{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Imitation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_name = \"Wall\" # Name of the Unity environment binary to launch\n",
    "train_mode = True # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\tmin_wall_height -> 5.5\n",
      "\t\tmax_wall_height -> 6.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 16\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 6\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brain_info = env.brains['Brain']\n",
    "s_size = brain_info.state_space_size\n",
    "a_size = brain_info.action_space_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_states = np.zeros([0, s_size])\n",
    "p_actions = np.zeros([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_dict = env.reset(train_mode=True)\n",
    "num_agents = len(brain_dict['Brain'].agents)\n",
    "for step in range(num_steps):\n",
    "    new_action = np.random.randint(0, a_size, size=[num_agents, 1])\n",
    "    info = env.step(new_action)['Brain']\n",
    "    p_actions = np.append(p_actions, new_action, axis=0)\n",
    "    p_states = np.append(p_states, info.states, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Expert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './wall_imitation'\n",
    "\n",
    "save_data = pickle.load(open(data_path+\"/data.p\", \"rb\"))\n",
    "e_states = save_data[\"states\"][0:num_agents*num_steps]\n",
    "e_actions = save_data[\"actions\"][0:num_agents*num_steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_states = (p_states - np.mean(p_states)) / np.std(p_states)\n",
    "e_states = (e_states - np.mean(e_states)) / np.std(e_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrimiator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    def __init__(self, s_size, a_size, h_size, lr):\n",
    "        self.state_in_expert = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "        self.action_in_expert = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.action_expert = tf.one_hot(self.action_in_expert, a_size)\n",
    "        \n",
    "        self.state_in_policy = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "        self.action_in_policy = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.action_policy = tf.one_hot(self.action_in_policy, a_size)\n",
    "        self.s_size = s_size\n",
    "        self.h_size = h_size\n",
    "        self.lr = lr\n",
    "        self.update()\n",
    "        \n",
    "    def get_d(self, state_in, action_in, reuse):\n",
    "        with tf.variable_scope(\"discriminator\"):\n",
    "            concat_input = tf.concat([state_in, action_in], axis=1)\n",
    "            hidden_1 = tf.layers.dense(concat_input, self.h_size, activation=tf.nn.tanh, use_bias=False, name=\"d_hidden_1\", reuse=reuse)\n",
    "            hidden_2 = tf.layers.dense(hidden_1, self.h_size, activation=tf.nn.tanh, use_bias=False, name=\"d_hidden_2\", reuse=reuse)\n",
    "            d = tf.layers.dense(hidden_2, 1, activation=tf.nn.sigmoid, use_bias=False, name=\"d_out\", reuse=reuse)\n",
    "            return d\n",
    "        \n",
    "    def update(self):\n",
    "        self.d_expert = self.get_d(self.state_in_expert, self.action_expert, False)\n",
    "        self.d_policy = self.get_d(self.state_in_policy, self.action_policy, True)\n",
    "        self.de = tf.reduce_mean(self.d_expert)\n",
    "        self.dp = tf.reduce_mean(self.d_policy)\n",
    "        self.d_loss = -tf.reduce_mean(tf.log(self.d_expert + 1e-10) + tf.log(1 - self.d_policy + 1e-10))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        self.update_batch = optimizer.minimize(self.d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epoch = 10\n",
    "s_size = 16\n",
    "a_size = 5\n",
    "h_size = 32\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "disc = Discriminator(s_size, a_size, h_size, lr)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_ordering(array):\n",
    "    s = np.arange(len(array))\n",
    "    np.random.shuffle(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675583 0.331891 0.655162\n",
      "0.851769 0.147071 0.15693\n",
      "0.96025 0.0396672 0.0444466\n",
      "0.985637 0.0143886 0.130539\n",
      "0.992664 0.00730666 0.00832548\n",
      "0.995493 0.00449424 0.00393074\n",
      "0.99703 0.0029626 0.00323037\n",
      "0.998022 0.00198373 0.00258053\n",
      "0.998582 0.00139701 0.00116604\n",
      "0.998936 0.000997169 0.000661084\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epoch):\n",
    "    e_s = random_ordering(e_states)\n",
    "    e_shuffle_states = e_states[e_s]\n",
    "    e_shuffle_actions = e_actions[e_s]\n",
    "    e_batch_reward = []\n",
    "    \n",
    "    p_s = random_ordering(p_states)\n",
    "    p_shuffle_states = p_states[p_s]\n",
    "    p_shuffle_actions = p_actions[p_s]\n",
    "    p_batch_reward = []\n",
    "\n",
    "    for j in range(len(p_states)//batch_size):\n",
    "        e_batch_states = e_shuffle_states[j*batch_size:(j+1)*batch_size]\n",
    "        e_batch_actions = e_shuffle_actions[j*batch_size:(j+1)*batch_size]\n",
    "        e_batch_actions = np.reshape(e_batch_actions, [-1])\n",
    "        \n",
    "        p_batch_states = p_shuffle_states[j*batch_size:(j+1)*batch_size]\n",
    "        p_batch_actions = p_shuffle_actions[j*batch_size:(j+1)*batch_size]\n",
    "        p_batch_actions = np.reshape(p_batch_actions, [-1])\n",
    "        \n",
    "        fd = {disc.state_in_expert: e_batch_states, disc.state_in_policy: p_batch_states,\n",
    "              disc.action_in_expert: e_batch_actions, disc.action_in_policy: p_batch_actions}\n",
    "        d_e, d_p, loss, _ = sess.run([disc.de, disc.dp, disc.d_loss, disc.update_batch], feed_dict=fd)\n",
    "        e_batch_reward.append(d_e)\n",
    "        p_batch_reward.append(d_p)\n",
    "    print(np.mean(e_batch_reward), np.mean(p_batch_reward), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
