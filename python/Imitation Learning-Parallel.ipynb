{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Imitation Learning (Parallel Behavioral Cloning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set environment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_name = \"race_i\" # Name of the Unity environment binary to launch\n",
    "train_mode = True # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brain_dict = env.reset(train_mode=False)\n",
    "E = brain_dict['BrainE']\n",
    "P = brain_dict['BrainP']\n",
    "\n",
    "brain_info = env.brains['BrainE']\n",
    "s_size = brain_info.state_space_size * brain_info.stacked_states\n",
    "a_size = brain_info.action_space_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImitationNN(object):\n",
    "    def __init__(self, state_size, action_size, h_size, lr, action_type):\n",
    "        self.state = tf.placeholder(shape=[None, state_size], dtype=tf.float32)\n",
    "        self.h_1 = tf.layers.dense(self.state, h_size, activation=tf.nn.elu)\n",
    "        self.h_2 = tf.layers.dense(self.h_1, h_size, activation=tf.nn.elu)\n",
    "        self.h_3 = tf.layers.dense(self.h_2, h_size, activation=tf.nn.elu)\n",
    "        self.h_4 = tf.layers.dense(self.h_3, h_size, activation=tf.nn.elu)\n",
    "        self.h_4d = tf.layers.dropout(self.h_4, 0.5)\n",
    "        self.logits = tf.layers.dense(self.h_4d, action_size, activation=None)\n",
    "        \n",
    "        if (action_type == \"discrete\"):\n",
    "            self.action_probs = tf.nn.softmax(self.logits)\n",
    "            self.sample_action = tf.multinomial(self.logits, 1)\n",
    "            self.true_action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.action_oh = tf.one_hot(self.true_action, action_size)\n",
    "            self.loss = tf.reduce_sum(-tf.log(self.action_probs + 1e-10)*self.action_oh)\n",
    "        \n",
    "            self.action_percent = tf.reduce_mean(tf.cast(\n",
    "                tf.equal(tf.cast(tf.argmax(self.action_probs, axis=1), tf.int32),\n",
    "                         self.action), tf.float32))\n",
    "        else:\n",
    "            self.sample_action = self.logits\n",
    "            self.true_action = tf.placeholder(shape=[None, action_size], dtype=tf.float32)\n",
    "            self.loss = tf.reduce_sum(tf.squared_difference(self.true_action, self.sample_action))\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "network = ImitationNN(s_size, a_size, 128, 1e-4, brain_info.action_space_type)\n",
    "\n",
    "num_steps = 2000\n",
    "batch_size = 64\n",
    "test_episodes = 1\n",
    "test_frequency = 10\n",
    "fast_testing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "percentages = []\n",
    "all_rewards = []\n",
    "\n",
    "\n",
    "sess.run(init)\n",
    "test_rewards = []\n",
    "brain_dict = env.reset(train_mode=fast_testing)\n",
    "E = brain_dict['BrainE']\n",
    "P = brain_dict['BrainP']\n",
    "\n",
    "expert_states = np.zeros([0, s_size])\n",
    "expert_states = np.append(expert_states, P.states, axis=0)\n",
    "expert_actions = np.zeros([0, 1])\n",
    "rewards = 0\n",
    "for i in range(num_steps):\n",
    "    agent_action = sess.run(network.sample_action, feed_dict={network.state: E.states})\n",
    "    brains_1 = env.step(agent_action[0])\n",
    "    E_1 = brains_1['BrainE']\n",
    "    P_1 = brains_1['BrainP']\n",
    "    expert_actions = np.append(expert_actions, P_1.previous_actions, axis=0)\n",
    "    rewards += E_1.rewards[0]\n",
    "    if len(expert_actions) > 1:\n",
    "        s = np.arange(len(expert_states))\n",
    "        np.random.shuffle(s)\n",
    "        shuffle_states = expert_states[s]\n",
    "        shuffle_actions = expert_actions[s]\n",
    "        batch_losses = []\n",
    "        for j in range(min(len(expert_states)//batch_size, 25)):\n",
    "            batch_states = shuffle_states[j*batch_size:(j+1)*batch_size]\n",
    "            batch_actions = shuffle_actions[j*batch_size:(j+1)*batch_size]\n",
    "            if brain_info.action_space_type == \"discrete\":\n",
    "                feed_dict = {network.state: batch_states, network.true_action:np.reshape(batch_actions, -1)}\n",
    "            else:\n",
    "                feed_dict = {network.state: batch_states, network.true_action:batch_actions}\n",
    "            loss, _ = sess.run([network.loss, network.update], feed_dict=feed_dict)\n",
    "            batch_losses.append(loss)\n",
    "        losses.append(np.mean(batch_losses))\n",
    "    expert_states = np.append(expert_states, P.states, axis=0)\n",
    "    E = E_1\n",
    "    P = P_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Close the environment when finished\n",
    "When we are finished using an environment, we can close it with the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
